<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Time series Forecasting with Pytorch Forecasting and Darts | Mateo Ávila Pava </title> <meta name="author" content="Mateo Ávila Pava"> <meta name="description" content="Advanced deep learning models, such as Temporal Fusion Transformers (TFT), are highly effective for complex multivariate time series forecasting, offering significant performance improvements. TFT is user friendly in libraries like PyTorch Forecasting or Darts, providing essential functions for plotting and interpreting predictions. When combined with tools like TensorBoard, TensorFlow Data Validation, and FeatureWiz, a flexible pipeline can be created to process, feature-engineer, and predict time series data, making it ideal for datasets like Rossmann store sales, where historical sales, promotions, and client information are leveraged for accurate forecasts.."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?cd27c99da1bef00e27636be155e78309"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://maavilapa.github.io/projects/3_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Mateo</span> Ávila Pava </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Time series Forecasting with Pytorch Forecasting and Darts</h1> <p class="post-description">Advanced deep learning models, such as Temporal Fusion Transformers (TFT), are highly effective for complex multivariate time series forecasting, offering significant performance improvements. TFT is user friendly in libraries like PyTorch Forecasting or Darts, providing essential functions for plotting and interpreting predictions. When combined with tools like TensorBoard, TensorFlow Data Validation, and FeatureWiz, a flexible pipeline can be created to process, feature-engineer, and predict time series data, making it ideal for datasets like Rossmann store sales, where historical sales, promotions, and client information are leveraged for accurate forecasts..</p> </header> <article> <h1 align="center">Time series forecasting with Temporal Fusion Transformer (TFT) in Rossman store sales dataset</h1> <p align="center"> <img src="assets/img/project2/fig_8.jpg" width="1300"> </p> <h2 id="business-case">Business case</h2> <p>Most advanced methods applied on time series include deep learning models, specially for complex multivariate forecasting problems. One of them, the Temporal fusion transformers (TFT), has demonstrated significant performance improvements over existing benchmarks and is currently one of the most accurate methods in forecasting. Although it is an advanced model, its implementation in the pytorch_forecasting library is user friendly and provides all the necessary functions to plot and interpret the model predictions. Besides, if it is combined with tools like tensorboard, tensorflow data validation and featurewiz, we can create a pipeline to prepare, add features and predict data in time series datasets in a flexible and understandable way.</p> <p>I will show this process using the Rossman store sales dataset, one of the open timeseries datasets available in Kaggle. We are provided with historical sales data for 1,115 Rossmann stores, using not just the sales historical of each store, but information about promotions, number of clients and holidays. Some stores in the dataset were temporarily closed for refurbishment and therefore, we have to clean the data and fill the missing values. The data frequency is daily and we have to predict the “Sales” for some of the stores given in the test set in the next 48 days. This is an interesting and useful AI application field because sellers could take advantage of this kind of predictions during their inventory planning, particularly when a lot of data about products sales and promotions in each store is available.</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_0.png" width="600"> </p> <h2 id="table-of-contents">Table of contents</h2> <details open=""> <a name="Table_contents"></a> <summary>Show/Hide</summary> <br> 1. [ File Descriptions ](#File_Description) 2. [ Technologies Used ](#Technologies_Used) 3. [ Summary ](#Summary) * [ 1. EDA and Cleaning ](#EDA_and_Cleaning) * [ Checking data types and missing values](#Checking_data_types_and_missing_values) * [ Filling missing values](#Filling_missing_values) * [ 2. Preprocessing ](#Preprocessing) * [ Scaling](#Scaling) * [ Create test dataframe](#Create_test_dataframe) * [ Date features](#Date_features) * [ 3. Training ](#Training) * [ Training parameters](#Training_parameters) * [ Create datasets](#Create_datasets) * [ Hyperparameter tuning](#Hyperparameter_tuning) * [ Predictions on validation data](#Predictions_on_validation_data) * [ Training and validation plots](#Training_and_validation_plots) * [ Interpret output](#Interpret_output) * [ Predict on test data](#Predict_on_test_data) * [ 4. Future improvements](#Future_improvements) </details> <h2 id="file-descriptions">File descriptions</h2> <details> <a name="File_Description"></a> <summary>Show/Hide</summary> <br> * <strong>[ data ](https://github.com/maavilapa/TemporalFusionTransformerExample/data)</strong>: folder containing all data files * <strong>sample_submission.csv</strong>: a sample submission file in the correct format * <strong>store.csv</strong>: supplemental information about the stores * <strong>test.csv</strong>: historical data excluding Sales * <strong>train.csv</strong>: historical data including Sales * <strong>[ images ](https://github.com/maavilapa/TemporalFusionTransformerExample/images)</strong>: folder containing images used for README and preparation notebook * <strong>[ preparation](https://github.com/maavilapa/TemporalFusionTransformerExample/preparation)</strong>: Functions used in the data preparation notebook used for preprocessing and training the model. * <strong>[ 1-2. Data preparation](https://github.com/maavilapa/TemporalFusionTransformerExample/1-2._Data_preparation.ipynb)</strong>: Notebook with all the data preparation, model training and predictions process. </details> <h2 id="technologies-used">Technologies used</h2> <details> <a name="Technologies_Used"></a> <summary>Show/Hide</summary> <br> * <strong>Python</strong> * <strong>Pandas</strong> * <strong>Numpy</strong> * <strong>Matplotlib</strong> * <strong>tensorflow</strong> * <strong>tensorboard</strong> * <strong>pytorch_lightning</strong> * <strong>Scikit-Learn</strong> * <strong>pytorch_forecasting</strong> </details> <h2 id="summary">Summary</h2> <p><a name="Summary"></a> <br></p> <h3 id="1-eda-and-cleaning">1. EDA and Cleaning</h3> <p><a name="EDA_and_Cleaning"></a></p> <h4 id="11-checking-data-types-and-missing-values">1.1. Checking data types and missing values</h4> <p><a name="Checking_data_types_and_missing_values"></a></p> <p>The first step is to import the necessary libraries defined in the requirements.txt file and to download the train.csv and store.csv files as pandas dataframes. Using tensorflow data validation we could check the data type for each column in both Train and Store dataframes, the percentage of zeros, missing values and more statistical information of each column.</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_1.PNG" width="600"> </p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_2.PNG" width="600"> </p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_3.PNG" width="600"> </p> <p>For categorical features we can check not just the missing values, but also the Top value and the unique values.</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_4.PNG" width="600"> </p> <p>We found that Promo2SinceYear, Promo2SinceWeek, CompetitionOpenSinceMonth, CompetitionOpenSinceYear and PromoInterval have more than 30% of missing values. CompetitionDistance has 0.26% of missing values. We will drop the first two columns and fill the missing values in the CompetitionDistance column to use it as a training feature. We merge the Train and Store dataframes. We check the number of missing values by columns using the next line:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_data.isnull<span class="o">()</span>.sum<span class="o">()</span>
</code></pre></div></div> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_4a.PNG" width="300"> </p> <p>We check that there are 2642 missing values of the 1017209 rows and records. If we look at which are the stores with missing values we find the following 3 stores: <strong>[291 622 879]</strong>. <br></p> <h4 id="12-filling-missing-values">1.2. Filling missing values</h4> <p><a name="Filling_missing_values"></a></p> <p>We first fill the CompetitionDistance Null values using the median Competition distance of the other 1112 stores. To do that we run the next line:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_data.CompetitionDistance<span class="o">=</span>train_data.CompetitionDistance.fillna<span class="o">(</span>train_data.CompetitionDistance.median<span class="o">())</span>
</code></pre></div></div> <p>It is necessary to check if a store has more than one Sales record for each unique date and if each date has a sales value for each store, since by default pytorch forecasting timeseries datasets allow missing timesteps but fill them with 0. We use the pandas duplicated, group by and value_counts functions to check the number of days recorded by store.</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig4_b.PNG" width="550"> </p> <p>We found that from 1115 stores, 934 have sales recorded for all the 942 days considered in the train data, while 180 Stores have just 758 sales recorded and 1 has 941 days. We filter the stores that have missing sales values and can plot the historical sales for some of these stores.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_data[[<span class="s2">"Date"</span>, <span class="s2">"Store"</span>, <span class="s2">"Sales"</span><span class="o">]][</span>train_data.Store<span class="o">==</span>539].set_index<span class="o">(</span><span class="s2">"Date"</span><span class="o">)</span>.plot<span class="o">()</span>
</code></pre></div></div> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_4c.PNG" width="450"> </p> <p>According to the plot, for some reason there were no sales for this store between July, 2014 and january 2015. This was the case for all the 180 stores closed for refurbishment with 184 missing values. We reindex the dates from the start date (2013-01-01) to the end date recorded (2015-07-31) so that all stores have the same dates. After we have expanded the dataset with the missing dates we have 1050330 rows and we check again the columns with missing values:</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_4d.PNG" width="300"> </p> <p>We impute the empty sales data with the same values of the last year (July 2013-January 2014). For the columns “CompetitionDistance”, “StoreType” and “Assortment” we take the last values of each store, since these are static variables. First, it is necessary to create new columns with the last year sales using the shift function and then we fill the missing sales, customers, open, promo and holidays values with the values of the previous year columns. We plot the sales for the same store after filling:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_data[[<span class="s2">"Date"</span>, <span class="s2">"Store"</span>, <span class="s2">"Sales"</span><span class="o">]][</span>train_data.Store<span class="o">==</span>539].set_index<span class="o">(</span><span class="s2">"Date"</span><span class="o">)[[</span><span class="s2">"Sales"</span><span class="o">]]</span>.plot<span class="o">()</span>
</code></pre></div></div> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_4e.PNG" width="450"> </p> <p>This way we verify that all the stores have all their historical data filled. <br></p> <h3 id="2-preprocessing">2. Preprocessing</h3> <p><a name="Preprocessing"></a></p> <p>The next step after filling and cleaning the data is to normalize the target and to add new features that could help improving the accuracy of the predictions. Since the forecast_horizon is 48 days and we already have the data for each day, we don’t have to resample the dataframe to weekly or monthly sales.</p> <h4 id="21-scaling">2.1. Scaling</h4> <p><a name="Scaling"></a> We first scale the sales for each store between -1 a 1 with the min max scaler of Scikit learn. Now if we plot the sales for the same store we see that the range is between -1 a 1.</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_4f.PNG" width="450"> </p> <p>We add a time_idx column necessary for training with temporal fusion transformer, beggining with 0 as the time index for the start date (2013-01-01) and (2015-09-17) as the end date (index 989).</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_4g.PNG" width="200"> </p> <h4 id="22-create-test-dataframe">2.2. Create test dataframe</h4> <p><a name="Create_test_dataframe"></a> We check if all the stores have the same prediction length for the test data. Then we merge the store columns with the test data and fill the missing values of the Open column using a dictionary for weekdays and weekends.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">open_dict</span><span class="o">={</span>1:1,2:1,3:1,4:1,5:1,6:1,7:0<span class="o">}</span>
test_data.Open<span class="o">=</span>test_data.Open.fillna<span class="o">(</span>test_data.DayOfWeek.map<span class="o">(</span>open_dict<span class="o">))</span>
</code></pre></div></div> <h4 id="23-date-features">2.3. Date features</h4> <p><a name="Date_features"></a> One of the main advantages of TFT is that it supports mixed covariates (includes past covariates known like sales promotions and weather features, and future covariates like temporal features, holidays and StoreOpen column). Therefore, we use the Featurewiz library to add data features like ‘quarter’, ‘is_summer’, ‘is_winter’, ‘dayofmonth’ and ‘weekofyear’, since these features could help the model recognize trends and seasonalities.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data, ts_adds_in <span class="o">=</span> FW.FE_create_time_series_features<span class="o">(</span>data, ts_column, <span class="nv">ts_adds_in</span><span class="o">=[])</span>
</code></pre></div></div> <p><br></p> <h3 id="3-training">3. Training</h3> <p><a name="Training"></a> <br></p> <h4 id="31-training-parameters">3.1. Training parameters</h4> <p><a name="Training_parameters"></a> We use a dictionary called params_dict to set the training parameters and then we split the data into test and train datasets. TFT implementation in pytorch_forecasting allows us to add categorical features by encoding them with the scikit’s learn default LabelEncoder. To define the timeseries dataset class we have to specify which variables are numerical or categorical and known in the future (like date features or sales promotions) and which are numerical or categorical but unknown in the future (like the number of customers in the store). The test data given in kaggle shows the next 48 days as the prediction interval, so we define our forecast_horizon (decoder length) as 48 and the input_window, wich represents the lenght of the TFT encoder, as 96 days. This means that the model will use a window of 96 days to predict the next 48 days. As a rule it is better to use an input window greater than the forecast horizon.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">params_dict</span><span class="o">={</span><span class="s2">"forecast_horizon"</span>:48,
             <span class="s2">"input_window"</span>:96,
             <span class="s2">"batch_size"</span>:16,
             <span class="s2">"group_ids"</span>:[<span class="s2">"Store"</span><span class="o">]</span>,
             <span class="s2">"unknown_reals"</span>:[<span class="s2">"Sales"</span>, <span class="s2">"Customers"</span><span class="o">]</span>,
             <span class="s2">"known_reals"</span>:[<span class="s2">"time_idx"</span>, <span class="s1">'Date_quarter'</span>,<span class="s1">'Date_dayofmonth'</span>, <span class="s1">'Date_weekofyear'</span><span class="o">]</span> ,
              <span class="s2">"static_reals"</span>:[<span class="s2">"CompetitionDistance"</span><span class="o">]</span>,
             <span class="s2">"static_categoricals"</span>:[<span class="s2">"StoreType"</span>, <span class="s2">"Assortment"</span><span class="o">]</span>,
             <span class="s2">"known_categoricals"</span>:[ <span class="s1">'Date_is_summer'</span>, <span class="s1">'Date_is_winter'</span>,<span class="s2">"Mes"</span>,<span class="s1">'DayOfWeek'</span>,<span class="s2">"Open"</span>, <span class="s2">"Promo"</span>, <span class="s2">"StateHoliday"</span>, <span class="s2">"SchoolHoliday"</span><span class="o">]</span>,
             <span class="s2">"target"</span>:<span class="s2">"Sales"</span>,
             <span class="s2">"unknown_categoricals"</span>:[],
             <span class="o">}</span>
<span class="nv">n_stores</span><span class="o">=</span>data.Store.nunique<span class="o">()</span>
</code></pre></div></div> <h4 id="32-create-datasets">3.2. Create datasets</h4> <p><a name="Create_datasets"></a> In the main training function, we create the training and validation dataloaders in the pytorch forecasting format and make some baseline predictions. The training_cutoff is the time index where the dataset is going to be split into training and validation sets. In this case, we take the last 48 days as validation dataset and leave the rest days for training. To check if the datasets were configured correctly we can look at the data parameter of each dataset, which is a dictionary that contains the different parameters previously defined as tensors. We check that the last value of the time_idx for the training dataset is 893 in this case.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>training.data
</code></pre></div></div> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_4h.PNG" width="700"> </p> <p>We use the baseline model of pytorch forecasting that uses last known target value to make predictions and calculate baseline performance in terms of mean absolute error (MAE):</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>actuals <span class="o">=</span> torch.cat<span class="o">([</span>y <span class="k">for </span>x, <span class="o">(</span>y, weight<span class="o">)</span> <span class="k">in </span>iter<span class="o">(</span>val_dataloader<span class="o">)])</span>
baseline_predictions <span class="o">=</span> Baseline<span class="o">()</span>.predict<span class="o">(</span>val_dataloader<span class="o">)</span>
print<span class="o">(</span><span class="s2">"Baseline error: "</span>,<span class="o">(</span>actuals - baseline_predictions<span class="o">)</span>.abs<span class="o">()</span>.mean<span class="o">()</span>.item<span class="o">())</span>
print<span class="o">(</span><span class="s2">"Baseline error median: "</span>,<span class="o">(</span>actuals - baseline_predictions<span class="o">)</span>.abs<span class="o">()</span>.median<span class="o">()</span>.item<span class="o">())</span>
</code></pre></div></div> <p align="center"> <strong>Baseline error:</strong> 0.3329482078552246 </p> <p align="center"> <strong>Baseline error median:</strong> 0.24794165790081024 </p> <h4 id="33-hyperparameter-tuning">3.3. Hyperparameter tuning</h4> <p><a name="Hyperparameter_tuning"></a> We tune the temporal fusion transformer using 50 epochs and 15 trials, using the next possible range of hyperparameters (the ones set by default):</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_4i.PNG" width="400"> </p> <p>After 3 hours it finishes and with <strong>verbose=1</strong> it will show after each trial the validation loss and the hyperparameters used for that trial. The models trained for the epoch with the lowest validation loss are saved in this case in the default folder <strong>lightning_logs</strong>, but you can use the name you want. For this case, the best 4 models are the first 4, particularly the trial 2 that has a validation loss of 0.01893.</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_5.PNG" width="1000"> </p> <h4 id="34-predictions-on-validation-data">3.4. Predictions on validation data</h4> <p><a name="Predictions_on_validation_data"></a> We load the best model trained using the load_from_checkpoint function at the epoch where the validaion loss was the lowest and we use the predict function in raw mode to get not only the predictions but also the attention given to the time indexes in the validation dataset and the real values for each store sales in this dataset. Besides, we plot the predictions of the validation set vs the real sales for each score and save these plots in the logs folder so we can check the results for each store in tensorboard.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">best_model_path</span><span class="o">=</span><span class="s2">"lightning_logs/trial_"</span>+str<span class="o">(</span>study.best_trial.number<span class="o">)</span>+<span class="s2">"/"</span>+os.listdir<span class="o">(</span><span class="s2">"lightning_logs/trial_"</span>+str<span class="o">(</span>study.best_trial.number<span class="o">))[</span>0]
best_tft <span class="o">=</span> TemporalFusionTransformer.load_from_checkpoint<span class="o">(</span>best_model_path<span class="o">)</span>     
<span class="c"># raw predictions are a dictionary from which all kind of information including quantiles can be extracted</span>
raw_predictions,x <span class="o">=</span> best_tft.predict<span class="o">(</span>val_dataloader, <span class="nv">mode</span><span class="o">=</span><span class="s2">"raw"</span>, <span class="nv">return_x</span><span class="o">=</span>True<span class="o">)</span>
with file_writer.as_default<span class="o">()</span>:
  <span class="k">for </span>idx, item <span class="k">in </span>enumerate<span class="o">(</span>data.Store.unique<span class="o">())</span>:
    tf.summary.image<span class="o">(</span><span class="s2">"Prediction_"</span>+str<span class="o">(</span>item<span class="o">)</span>, preparation.plot_to_image<span class="o">(</span>best_tft.plot_prediction<span class="o">(</span>x, raw_predictions, <span class="nv">idx</span><span class="o">=</span>idx, <span class="nv">add_loss_to_title</span><span class="o">=</span>True<span class="o">))</span>, <span class="nv">step</span><span class="o">=</span>0<span class="o">)</span>
<span class="nv">aux</span><span class="o">=</span>pd.DataFrame<span class="o">(</span>raw_predictions[<span class="s2">"prediction"</span><span class="o">]</span>.numpy<span class="o">()</span>.reshape<span class="o">(</span>n_stores<span class="k">*</span>params_dict[<span class="s2">"forecast_horizon"</span><span class="o">]</span> , 7<span class="o">))</span>
</code></pre></div></div> <p>Now we calculate the mean absolute error and median absolute error on validation set and compare them with the baseline errors.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print<span class="o">(</span><span class="s2">"Error mean: "</span>,<span class="o">(</span>actuals <span class="nt">-aux</span><span class="o">[</span>3].values.reshape<span class="o">(</span>n_stores,params_dict[<span class="s2">"forecast_horizon"</span><span class="o">]))</span>.abs<span class="o">()</span>.mean<span class="o">())</span>
print<span class="o">(</span><span class="s2">"Error median: "</span>,<span class="o">(</span>actuals <span class="nt">-aux</span><span class="o">[</span>3].values.reshape<span class="o">(</span>n_stores,params_dict[<span class="s2">"forecast_horizon"</span><span class="o">]))</span>.abs<span class="o">()</span>.median<span class="o">())</span>
</code></pre></div></div> <p align="center"> <strong>Error mean: </strong> tensor(0.0690) </p> <p align="center"> <strong>Error median: </strong> tensor(0.0477) </p> <h4 id="35-training-and-validation-plots">3.5. Training and validation plots</h4> <p><a name="Training_and_validation_plots"></a> If you want to check in the notebook the best hyperparameters found during the hyperparameter tuning you just have to execute the next line:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print<span class="o">(</span>study.best_trial.params<span class="o">)</span>
</code></pre></div></div> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_6.PNG" width="1100"> </p> <p>With tensorboard we can also check the hyperparameters and plot the validation and train loss metrics vs each epoch for all the models trained. To open tensorboard in colab, it is necessary to type two magic commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%load_ext tensorboard
%tensorboard <span class="nt">--logdir</span> lightning_logs
</code></pre></div></div> <p>Where lightning_logs is the folder with the training logs defined in the optimize_hyperparameters function. With the help of tensorboard we can compare the validation loss curves for the best four models. In our case the model was the version 2 shown in light blue color. It is also possible to plot the Validation or training MAPE, RMSE, SMAPE and MAE. From this curves we can see that the lowest validation loss is reached close to the 50 epochs or 5000 steps.</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/val_loss.svg" width="500"> </p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_7a.PNG" width="500"> </p> <p>We could make the same analysis for the training loss in order to check if the model is overfitting or having some problems while training.</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/train_loss_epoch.svg" width="500"> </p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_7b.PNG" width="500"> </p> <p>From both validation and training loss curves we see that in both cases the best model is the version_2 model. Both losses improved after each epoch and the gap between the curves at the last epoch is close to 0.002, so we check that the model is not overfitting and that probably we could improve the results by using more epochs and a callback like a learning rate scheduler. Since we are also tuning the learning rate, we could explore the learning rate behavior for each model.</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/lr-Ranger.svg" width="600"> </p> <p>The learning rate remained constant throughout the training for all models, although each one has a different lr value. There is also available a table which compares the TFT hyperparameters used for each model trained, with the four best models underlined in red.</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_7.jpg" width="600"> </p> <p>Finally, one of the most important plot we can find in tensorboard thanks to the tensorflow and pytorch_forecasting functions is the validation predictions vs real sales for each one of the stores. We show the predictions for the stores 1, 1115 and 706, where the blue curves are the historical sales and the orange one are the predictions.</p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_8.jpg" width="1100"> </p> <p>From the predictions it can be seen that the model takes into account the days when the stores are closed as well as the trends for each day and week depending on the store. However, for some cases it underestimates sales, probably because the information from the second promotion was not included in the dataset.</p> <h4 id="36-interpret-output">3.6. Interpret output</h4> <p><a name="Interpret_output"></a> Temporal fusion transformer model has a Variable selection network and with the pytorch forecasting functions we can plot the importance of each Categorical and real feature both for the encoder and decoder. Besides, it has an Attention mechanism that decides which are the most important past time indexes to take into account during training and its plot is also included when we run the next to lines:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>interpretation <span class="o">=</span> best_tft.interpret_output<span class="o">(</span>raw_predictions, <span class="nv">reduction</span><span class="o">=</span><span class="s2">"sum"</span><span class="o">)</span>
best_tft.plot_interpretation<span class="o">(</span>interpretation<span class="o">)</span>
</code></pre></div></div> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_11.jpg" width="850"> </p> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_13.jpg" width="850"> </p> <p>From the first plot, which shows the attention given to each time index in the encoder, we can see which days were the most important in the sales history in general for all the stores. Regarding the encoder features, the most important is the sales columns, followed by the DayOfWeek, promotions and Open, which indicates if the store was open. In the decoder, the most important variables the Open flag, DayOfWeek and Promo columns. This was expected, since it is clear that when the store is closed there are no sales and that these sales depend a lot on the day of the week and promotions.</p> <h4 id="37-predict-on-test-data">3.7. Predict on test data</h4> <p><a name="Predict_on_test_data"></a> Then we take the same model and predict the sales for the next 48 days and stores defined in the test_data, save them in a dataframe called predict and add the Store and time_idx columns.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>raw_test_predictions, <span class="nv">x_test</span><span class="o">=</span>best_tft.predict<span class="o">(</span>test_data, <span class="nv">mode</span><span class="o">=</span><span class="s2">"raw"</span>, <span class="nv">return_x</span><span class="o">=</span>True<span class="o">)</span>
<span class="nv">predict</span><span class="o">=</span>pd.DataFrame<span class="o">(</span>raw_test_predictions[<span class="s2">"prediction"</span><span class="o">]</span>.numpy<span class="o">()</span>.reshape<span class="o">(</span>n_stores<span class="k">*</span>params_dict[<span class="s2">"forecast_horizon"</span><span class="o">]</span> , 7<span class="o">))</span> 
predict.columns<span class="o">=[</span><span class="s2">"p5"</span>,<span class="s2">"p20"</span>,<span class="s2">"p40"</span>,<span class="s2">"p50"</span>,<span class="s2">"p60"</span>,<span class="s2">"p80"</span>,<span class="s2">"p95"</span><span class="o">]</span>
predict.insert<span class="o">(</span>7, <span class="s2">"Store"</span>, np.repeat<span class="o">(</span>data.Store.unique<span class="o">()</span>, params_dict[<span class="s2">"forecast_horizon"</span><span class="o">]))</span>
predict.insert<span class="o">(</span>8, <span class="s2">"time_idx"</span>, np.tile<span class="o">(</span>np.arange<span class="o">(</span>data.time_idx.max<span class="o">()</span>+1,data.time_idx.max<span class="o">()</span>+1+params_dict[<span class="s2">"forecast_horizon"</span><span class="o">])</span>, n_stores<span class="o">))</span>
</code></pre></div></div> <p>We could plot some of the predictions of the test data as we did with the validation data:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for </span>idx <span class="k">in </span>range<span class="o">(</span>0,2<span class="o">)</span>: 
    best_tft.plot_prediction<span class="o">(</span>x_test, raw_test_predictions, <span class="nv">idx</span><span class="o">=</span>idx, <span class="nv">show_future_observed</span><span class="o">=</span>False<span class="o">)</span><span class="p">;</span>
</code></pre></div></div> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_15.jpg" width="850"> </p> <p>As we can see in the plots, the predictions are also scaled between -1 and 1, so we have to rescale them before the submission in Kaggle. We check now for the first store if the sales values are in the original train data scale.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>predict[predict.Store <span class="o">==</span> <span class="s2">"1.0"</span><span class="o">]</span>.set_index<span class="o">(</span><span class="s2">"timestamp"</span><span class="o">)[[</span><span class="s2">"p50"</span><span class="o">]]</span>.plot<span class="o">()</span>
</code></pre></div></div> <p align="center"> <img src="https://github.com/maavilapa/TemporalFusionTransformerExample/blob/main/images/fig_17.PNG" width="500"> </p> <p>The last step is to clean and format the prediction DataFrame to the sample_submission format given in Kaggle. To do that, we take only the positive predictions and set to zero the ones that are negatives, merge the predict dataframe with the test data Id and make the submission.</p> <h2 id="4-future-improvements">4. Future improvements</h2> <p><a name="Future_improvements"></a> With the preparation of the data and the training of the model, an accuracy of 0.10573 was achieved in the public test dataset, while the accuracy of the lead team is 0.08932. So this is a very good result and it can be further improved. The process that I explained is only one of the possible ways to prepare and train the dataset, and I explain it and teach it only for academic reasons. There are a few options we can try to improve accuracy:</p> <ul> <li>Take all the given columns in the Store dataset.</li> <li>Fill in the missing sales values in some stores not with the values of the previous year but with the values of previous months or using strategies such as Moving Average.</li> <li>Normalize the data using a different strategy.</li> <li>Add other external variables that can affect sales.</li> <li>Do hyperparameter tuning with more epochs or more trials.</li> </ul> <p><br> <strong>LinkedIn:</strong> www.linkedin.com/in/mateoavila</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Mateo Ávila Pava. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 20, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"List of main projects I have worked on",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-hexbug-head-tracking",title:"HexBug Head Tracking",description:"Framework to track a freely and randomly moving object (\u201cHexBug\u201d) head from a video, within the context of the Tracking Olympiad (TRACO) course.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-autonomous-robotic-product-picking-and-storage",title:"Autonomous Robotic Product Picking and Storage",description:"Design and evaluation of an object grasping plan using Generative grasping convolutional neural networks (GGCNN) for a robotic system for autonomous product picking and storage with the settings of the Amazon Picking Challenge (APC).",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-time-series-forecasting-with-pytorch-forecasting-and-darts",title:"Time series Forecasting with Pytorch Forecasting and Darts",description:"Advanced deep learning models, such as Temporal Fusion Transformers (TFT), are highly effective for complex multivariate time series forecasting, offering significant performance improvements. TFT is user friendly in libraries like PyTorch Forecasting or Darts, providing essential functions for plotting and interpreting predictions. When combined with tools like TensorBoard, TensorFlow Data Validation, and FeatureWiz, a flexible pipeline can be created to process, feature-engineer, and predict time series data, making it ideal for datasets like Rossmann store sales, where historical sales, promotions, and client information are leveraged for accurate forecasts..",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%61%74%65%6F.%61%76%69%6C%61@%66%61%75.%64%65","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0008-3061-8588# your ORCID ID","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=jpzBz8cAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/maavilapa","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/mateoavila# your LinkedIn user name","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>